
## 什么是时间戳?

时间戳是自一个特定时刻（称为“epoch”）起经过的时间量的表示。它在计算机科学中广泛用于记录事件发生的时间点，常用于各大日志、数据包等等。最常见的epoch就是Unix epoch，即1970年1月1日00:00:00 UTC。


## 1. 两种时间标准

   UTC 和 GMT 都是时间标准，定义事件的精度。它们只表示 零时区 的时间，本地时间则需要与 时区 或偏移 结合后表示。这两个标准之间差距通常不会超过一秒。

### UTC（协调世界时）

UTC，即协调世界时（Coordinated Universal Time），是一种基于原子钟的时间标准。它的校准是根据地球自转的变化而进行的，插入或删除闰秒的实际需求在短期内是难以预测的，因此这个决定通常是在需要校准的时候发布。 闰秒通常由国际电信联盟（ITU） 和国际度量衡局（BIPM） 等组织进行发布。由国际原子时（International Atomic Time，TAI） 通过闰秒 的调整来保持与地球自转的同步。

### GMT（格林尼治标准时间）

以英国伦敦附近的格林尼治天文台（0 度经线，本初子午线）的时间为基准。使用地球自转的平均速度来测量时间，是一种相对于太阳的平均时刻。尽管 GMT 仍然被广泛使用，但现代科学和国际标准更倾向于使用 UTC。

## 2. 两种显示标准
   上面我们讨论的时间标准主要保证的是时间的精度，时间显示标准指的是时间的字符串表示格式。我们熟知的有 RFC 5322 和 ISO 8601。

#### RFC 5322 电子邮件消息格式的规范

RFC 5322 的最新版本是在 2008 年 10 月在 IETF 发布的，你阅读时可能有了更新的版本。

RFC 5322 是一份由 Internet Engineering Task Force (IETF) 制定的标准，定义了 Internet 上的电子邮件消息的格式规范。该标准于 2008 年发布，是对之前的 RFC 2822 的更新和扩展。虽然 RFC 5322 主要关注电子邮件消息的格式，但其中的某些规范，比如日期时间格式，也被其他领域采纳，例如在 HTTP 协议中用作日期头部（Date Header）的表示。
格式通常如下:

Thu, 14 Dec 2023 05:36:56 GMT

时区部分为了可读可以如下表示：

```
Thu, 14 Dec 2023 05:36:56 CST
Thu, 14 Dec 2023 05:36:56 +0800
Thu, 14 Dec 2023 05:36:56 +0000
Thu, 14 Dec 2023 05:36:56 Z
```

但并不是所有程序都兼容这种时区格式，通常程序会忽略时区，在写程序时要做好测试。标准没有定义毫秒数如何显示。

需要注意的是，有时候我们会见到这种格式Tue Jan 19 2038 11:14:07 GMT+0800 (中国标准时间)，这是 js 日期对象转字符串的格式，它与标准无关，千万不要混淆了。

#### ISO 8601

ISO 8601 最新版本是 ISO 8601:2019，发布日期为 2019 年 11 月 15 日，你阅读时可能有了更新的版本。

下面列举一些格式示例：

```
2004-05-03T17:30:08+08:00
2004-05-03T17:30:08+00:00
2004-05-03T17:30:08Z
2004-05-03T17:30:08.000+08:00
```

标准并没有定义小数位数，保险起见秒后面一般是 3 位小数用来表示毫秒数。 字母 "Z" 是 "zero"（零）的缩写，因此它被用来表示零时区，也可以使用 + 00:00，但 Z 更直观且简洁。

本标准提供两种方法来表示时间：一种是只有数字的基础格式；第二种是添加了分隔符的扩展格式，更易读。扩展格式使用连字符 “-” 来分隔日期，使用冒号 “:” 来分隔时间。比如 2009 年 1 月 6 日在扩展格式中可以写成 "2009-01-06"，在基本格式中可以简单地写成 "20090106" 而不会产生歧义。 若要表示前 1 年之前或 9999 年之后的年份，标准也允许有共识的双方扩展表达方式。双方应事先规定增加的位数，并且年份前必须有正号 “+” 或负号 “-” 而不使用“。依据标准，若年份带符号，则前 1 年为 "+0000"，前 2 年为 "-0001"，依此类推。

午夜，一日的开始：完全表示为 000000 或 00:00:00；仅有小时和分表示为 0000 或 00:00

午夜，一日的终止：完全表示为 240000 或 24:00:00；仅有小时和分表示为 2400 或 24:00

如果时间在零时区，并恰好与 UTC 相同，那么在时间最后加一个大写字母 Z。Z 是相对协调世界时时间 0 偏移的代号。 如下午 2 点 30 分 5 秒表示为 14:30:05Z 或 143005Z；只表示小时和分，为 1430Z 或 14:30Z；只表示小时，则为 14Z 或 14Z。

其它时区用实际时间加时差表示，当时的 UTC+8 时间表示为 22:30:05+08:00 或 223005+0800，也可以简化成 223005+08。

日期与时间合并表示时，要在时间前面加一大写字母 T，如要表示东八区时间 2004 年 5 月 3 日下午 5 点 30 分 8 秒，可以写成 2004-05-03T17:30:08+08:00 或 20040503T173008+08。


在编写 API 时推荐使用 ISO 8601 标准接收参数或响应结果，并且做好时区测试，因为不同编程语言中实现可能有差异。

可以看到时间戳都是一串数字，对于人来说非常不好读的，因此需要有一个标准，将时间戳转换成可读的统一时间标准，其中之一就是 ISO-8601 标准。

ISO-8601 是一种国际标准化的日期和时间表示方法。这种格式旨在提供一种清晰、一致的方法来表示时间，易于人类阅读和机器解析。

ISO-8601 格式由如下几部分组成：

日期部分：按照 “YYYY-MM-DD” 格式。
时间部分：按照 “HH:MM:SS” 格式。
分隔符：日期和时间之间使用T。
时区：UTC 时间用Z表示，"Z" 是指 "Zulu time"，这是军事和航空领域中用于指代 UTC 的术语，在 ISO-8601 中，这个 "Z" 代表零时区；其他时区用与 UTC 的时差表示，如+HH:MM或-HH:MM。

比如2024-01-23T13:00:00+00:00Z表示 UTC 标准 (约等于 0 时区) 的 2014 年 1 月 23 日下午一点，对应我们中国的时间就是晚上 9 点；而2024-01-23T13:00:00+08:00表示东八区的 2024 年 1 月 23 下午 1 点。



### 协调世界时（UTC）

Coordinated Universal Time，使用原子钟来计算时间。

“Z”是协调世界时中 0 时区的标志。

“09:30 UTC”就写作“09:30Z”或是“0930Z”。

“14:45:15 UTC”则为“14:45:15Z”或“144515Z”。

时间戳/unix 时间戳
是从 1970 年 1 月 1 日（UTC/GMT 的午夜）开始所经过的秒数。

最初计算机操作系统是 32 位，而时间也是用 32 位表示，2147483647。 换算下来大概是 68 年，实际上到 2038 年 01 月 19 日 03 时 14 分 07 秒，便会到达最大时间。

因为用 32 位来表示时间的最大间隔是 68 年，而最早出现的 UNIX 操作系统考虑到计算机产生的年代和应用的时限综合取了 1970 年 1 月 1 日作为 UNIX TIME 的纪元时间。

但是不用担心，64 位操作系统可以表示到 292,277,026,596 年 12 月 4 日 15 时 30 分 08 秒，哪怕地球毁灭那天都不用愁不够用了。

**为什么是 1970 年 1 月 1 日？**

这个选择主要是出于历史和技术的考虑。

Unix 操作系统的设计者之一，肯 · 汤普森（Ken Thompson）和丹尼斯 · 里奇（Dennis Ritchie）在开发 Unix 操作系统时，需要选择一个固定的起始点来表示时间。1970-01-01 00:00:00 UTC 被选为起始时间。这个设计的简洁性和通用性使得 Unix 时间戳成为计算机系统中广泛使用的标准方式来表示和处理时间。

**为什么Unix epoch是1970年呢？**

选择1970年元旦的零时作为Unix epoch有历史原因，也有随机性，随便看了看后总结了如下几点：Unix操作系统首次发布是在1969年，然后需要一个简单而实用的方法来表示时间，因此大佬们休假回来一讨论，就把非常接近Unix 系统开发时间并且有初始化意义的1970年1月1日作为Unix时间戳的起始时间了。 因为是休完圣诞元旦假期回来的，1970年1月1日已经作为过去时了，作为一个过去式的接近操作系统开发时间的日期可以避免在表示当时及之后的日期时出现负数。 当时Unix时间戳还是用32位整数来存储的，这意味着它可以表示的最大值是 2^31-1 秒，这样从1970年往前往后算，可以覆盖1901年到2038奶奶的时间，当时来看基本够用了（32系统需要注意2038年问题）。 1970年1月1日就是一个普通的新年日，没有与现有历法或重要历史事件相关联，这使得它作为一个“中性”的起点非常合适，避免了不同文化上的认同问题。 最后使用 UTC 作为标准，是因为UTC作为“协调世界时”（Coordinated Universal Time），是目前国际上最广泛采用的时间标准。它是一种基于原子时钟的时间尺度，与格林威治平均时（GMT）非常接近，但在技术上更为准确。总体来说就是UNIX大概这个时间点发布的，过完年就拍脑门子定了。

**时间戳为什么只能表示到 2038 年 01 月 19 日 03 时 14 分 07 秒？**

在许多系统中，结构体time_t 被定义为 long，具体实现取决于编译器和操作系统的架构。例如，在 32 位系统上，time_t 可能是 32 位的 long，而在 64 位系统上，它可能是 64 位的 long。 32 位有符号 long 类型，实际表示整数只有 31 位，最大能表示十进制 2147483647（01111111 11111111 11111111 11111111）。

> new Date(2147483647000)
< Tue Jan 19 2038 11:14:07 GMT+0800 (中国标准时间)

实际上到 2038 年 01 月 19 日 03 时 14 分 07 秒，便会到达最大时间，过了这个时间点，所有 32 位操作系统时间便会变为 10000000 00000000 00000000 00000000。因具体实现不同，有可能会是 1901 年 12 月 13 日 20 时 45 分 52 秒，这样便会出现时间回归的现象，很多软件便会运行异常了。

至于时间回归的现象相信随着 64 为操作系统的产生逐渐得到解决，因为用 64 位操作系统可以表示到 292,277,026,596 年 12 月 4 日 15 时 30 分 08 秒。

另外，考虑时区因素，北京时间的时间戳的起始时间是 1970-01-01T08:00:00+08:00。


###太阳时间

#### 时区
时区是地理上的一个区域，在该区域遵守同一的标准时间。

地球是自西向东自转的，因此东边会比西边先看到太阳。从经验上讲，太阳升降是一天的时间周期。在这种时间度量方式下，时间就不是一个绝对的概念了。

不同经度地区存在时差，但严格区分实在太不方便了。实际生活中，不需要到分和秒那么精确，1 小时内的误差是可以被接受的。

因此，1884 年在华盛顿召开的国际子午线会议上，规定将全球划分为 24 个时区(东、西各十二个时区)，其中以英国格林尼治天文台旧址作为零时区，每个时区横跨经度 15 度，时间恰好为 1 小时，而东、西第 12 时区各跨经度 7.5 度，以东、西经 180 度为界。每个时区内时间，统一以该时区的中央经线的时间为主，相邻的两个时区间总是相差一个小时，这就是时区的由来。

事实上，时区的划分并不是一个严谨的事情，常见的情况是：一个国家或者一个省份同时跨着 2 个或者更多的时区。 以中国为例，差不多横跨 5 个时区，理论上在国内应该有 5 个时间，但为了使用起来方便，我们统一用一个时区。 东八区覆盖了我国的绝大多数人口，且覆盖了我国绝大多数重要城市。因此选取东八区为“中国时间”，又因为北京是我国首都，所以称之为“北京时间”。

### UTC 偏移

UTC 偏移量是协调世界时（UTC）和特定地点的日期与时间差异，其单位为小时和分钟。 它通常以 ±[hh]:[mm]、±[hh][mm]、或 ±[hh]的格式显示。 UTC+08:00 是比世界协调时间快 8 小时的时区。

**GMT**

Greenwich Mean Time 格林尼治标准时间。这个地方的当地时间过去被当成世界标准的时间。 GMT 是前世界标准时，UTC 是现世界标准时。UTC 比 GMT 更精准，以原子时计时，适应现代社会的精确计时。但在不需要精确到秒的情况下，二者可以视为等同。 格林尼治刚好在 0 时区上。

GMT = UTC+0

### 夏令时

DST（Daylight Saving Time），夏令时又称夏季时间，或者夏时制。使用夏令时的国家在夏季使用夏令时，标准时间也因此被相应地称为冬季时间。

是为节约能源而人为规定地方时间的制度。一般在天亮早的夏季人为将时间提前一小时，可以使人早起早睡，减少照明量，以充分利用光照资源，从而节约照明用电。

在施行夏令时的国家，一年里面有一天只有 23 小时（夏令时开始那一天），有一天有 25 小时（夏令时结束那一天），其他时间每天都是 24 小时。

从过去的 100 多年来看，夏令时往往是在国家发生严重危机（如战争和能源短缺）的情况下才会受到青睐。而在相对和平的近 10 年里，这种时间制度则变得越来越不受欢迎。它会使得人们的生物钟被扰乱，常常陷入睡眠不足的情况，不仅对人体健康有害、导致车祸，还会对旅游、航空领域造成极大的混乱。 另外，冬、夏令时究竟能否起到节能的作用，也仍有待商榷。美国一项截至 2014 年 3 月的研究表明，这种时间转换制度最多能在 3、4 月帮助美国减少 1%的用电量，而美国国家标准局则认为，夏令时对用电量没有丝毫影响。 在俄罗斯，此前的一份报告也显示，夏令时帮助俄罗斯每年节约的电量，仅相当于两三个火力发电厂的发电量，十分的“鸡肋”。

### 本地时间

在日常生活中所使用的时间我们通常称之为本地时间。 这个时间等于我们所在（或者所使用）时区内的当地时间，它由与世界标准时间（UTC）之间的偏移量来定义。 这个偏移量可以表示为 UTC- 或 UTC+，后面接上偏移的小时和分钟数。

### 时间格式

**ISO 8601**

国际标准 ISO 8601，是国际标准化组织的日期和时间的表示方法。

1970-01-01T00:00:00Z

年 YYYY
月 MM
日 DD
T 标识为 UTC
Z 标识时区为 0
instant（Java）
Instant.now()使用等是 UTC 时间

LocalDate、LocalDateTime 的 now()方法使用的是系统默认时区。

TIMESTAMP
存储时，MySQL 将 TIMESTAMP 值从当前时区转换为 UTC 时间进行存储，查询时，将数据从 UTC 转换为检索的当前时区。

默认情况下，每个连接的当前时区是服务器的时间。时区可以根据每个连接进行设置。只要时区设置保持不变，就可以得到存储的相同值。

只要写入和读出时的时区设置一致，就能获取到一致的值。

数据库连接时的时间转换
每次创建会话前，先取服务端的时区 STZ 和客户端时区 CTZ(比如 serverTimezone=GMT+8) 如果 CTZ 没有设置就尝试将 STZ 转换为客户端时区，作为当前会话的时区。

时间类型的处理发生在 SQL 参数绑定阶段，Java 日期时间类型被转换为 SQL 语句(字符串)

### 时间戳的精确度如何区分呢？

聊到时间戳，就得聊聊其精确度，平常我们看时间就是时分秒，但是在计算机或者更加高精尖的技术需求中，比如航天，秒已经不是最小单位了，需要更精确的毫秒甚至纳秒的精度。

时间戳可以精确到下边四种不同的级别：

``` 
秒：最基本的 Unix 时间戳是以秒为单位的，表示自 Unix epoch 以来的秒数，比如1970年1月1日00:00:01 UTC距离 Unix epoch 就差 1 秒，那么1970年1月1日00:00:01 UTC的时间戳就是 1。
毫秒：毫秒级时间戳是秒级时间戳的千分之一。
微秒：微秒级进一步细分为秒的百万分之一。
纳秒：纳秒级时间戳提供最高精度，为秒的十亿分之一。
```

区分秒级、毫秒级、微秒级和纳秒级时间戳主要依赖于它们的长度（位数）和数值范围：

``` 
秒级时间戳（Second-level Timestamp）的长度通常为 10 位数字。例如，1617181723。
毫秒级时间戳（Millisecond-level Timestamp）通常为 13 位数字。例如，1617181723000。
微秒级时间戳（Microsecond-level Timestamp）通常为 16 位数字。例如，1617181723000000。
纳秒级时间戳（Nanosecond-level Timestamp）可能超过 16 位，通常在 19 位左右，具体取决于时间的具体值。例如，1617181723000000000。
```
